#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Sep 15 23:53:53 2018

@author: vickywinter
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
from scipy import stats
import sklearn.model_selection
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import scale 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestRegressor
import xgboost
from sklearn.metrics import explained_variance_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV

#loading data
test=pd.read_csv('/Users/vickywinter/Documents/NYC/Machine Learning Proj/Data/all/test.csv')
train=pd.read_csv('/Users/vickywinter/Documents/NYC/Machine Learning Proj/Data/all/train.csv')
all_data=pd.concat([train,test])

# transfer bathroom and 2ndFlrSF to more reasonable variables
all_data['Bath']=all_data['FullBath']+all_data['HalfBath']*0.25
all_data['BsmtBath']=all_data['BsmtFullBath']+all_data['BsmtHalfBath']*0.25
all_data=all_data.drop(columns=['FullBath', 'HalfBath','BsmtFullBath','BsmtHalfBath'])
#all_data['floorNo']=np.where(all_data['2ndFlrSF']>0,2,1)
#all_data=all_data.drop(columns=['2ndFlrSF'])


#create corrlation map
train=all_data[all_data["Id"]<=1460]
test=all_data[all_data["Id"]>1460]
corrmat = train.corr()
plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);

corr=train.corr().abs()
s = corr.unstack()
so = s.sort_values(kind="quicksort")
#from the map we can see that the high (>70%) correlation pairs are
#Garage Area v.s. Garage Cars
#Garage Year Built v.s. YearBuilt
#TotRmsAbvGrd v.s GrLivArea
#1stFlrSF v.s. TotalBsmtSF

all_data=pd.concat([train,test])
# To avoid high correlate , drop or transfer pair into one columns
all_data=all_data.drop(columns=['GarageArea'])
all_data['HasGarage']=np.where(all_data['GarageYrBlt']>0,1,0)
all_data=all_data.drop(columns=['GarageYrBlt'])
all_data=all_data.drop(columns=['TotalBsmtSF'])

#handle missing value

missing_value=pd.DataFrame({'miss':all_data.isnull().sum(),'ratio':(all_data.isnull().sum() / len(all_data)) * 100})
missing_value=missing_value.sort_values(by=['miss'],ascending=False)
# missing_value shows the number and percentage of missing value for each variables



#according to the definiation, some missing value variables have specific meaning
for var in ("MasVnrType",'BsmtFinType1','BsmtFinType2','MasVnrType','BsmtFinType1','BsmtFinType2','BsmtQual','BsmtExposure','BsmtCond','GarageType','GarageQual','GarageCond','GarageFinish','Fence','FireplaceQu','Fence','Alley','MiscFeature','PoolQC'):
    all_data[var]=all_data[var].fillna("None")
    
all_data["Functional"] = all_data["Functional"].fillna("Typ")

#for numeric variables, replace missing value by 0
for var in ("MasVnrArea","BsmtBath",'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','Utilities'):
    all_data[var]=all_data[var].fillna(0)

#for category variables, replace missing valuw by most frequent value
for var in ('MSZoning','KitchenQual',"Exterior1st","Exterior2nd",'SaleType','Electrical'):
    all_data[var] = all_data[var].fillna(all_data[var].mode()[0])


#LotFrontage 16.64% missing value, since each neighborhood has similar lot value, replace
#the missing value with the median value in each neighborhood, same with GarageArea
all_data["LotFrontage"] = all_data.groupby("Neighborhood")["LotFrontage"].transform(lambda x: x.fillna(x.median()))

# According to the description the NA GarageTy means no Garage
#all_data["GarageYrBlt"] = all_data["GarageYrBlt"].fillna(lambda row: 0 if row['GarageType']=='None' else row["GarageYrBlt"])
#all_data["GarageArea"] = all_data["GarageArea"].fillna(lambda row: 0 if row['GarageType']=='None' else row["GarageArea"])
all_data["GarageCars"] = all_data["GarageCars"].fillna(lambda row: 0 if row['GarageType']=='None' else row["GarageCars"])
all_data["GarageCars"] = all_data["GarageCars"].convert_objects(convert_numeric=True)


missing_value=pd.DataFrame({'miss':all_data.isnull().sum(),'ratio':(all_data.isnull().sum() / len(all_data)) * 100})
missing_value=missing_value.sort_values(by=['miss'],ascending=False)


train=all_data[all_data["Id"]<=1460]
test=all_data[all_data["Id"]>1460]




corrmat = train.corr()
plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);

corr=train.corr().abs()
s = corr.unstack()
so = s.sort_values(kind="quicksort")

k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index
cm = np.corrcoef(train[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

#SalesPrice: OverallQual, GrLivArea, GarageCars,1stFlrSF, Bath, TotRmsAbvGrd, YearBuilt, YearRemodAdd,MasVnrArea



# Find outlier use most correlated variables and delete them 
sns.set()
cols=['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', '1stFlrSF','Bath', 'TotRmsAbvGrd','YearBuilt','YearRemodAdd','MasVnrArea']
sns.pairplot(train[cols], size = 2.5)
plt.show();

train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)

sns.set()
cols=['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', '1stFlrSF','Bath', 'TotRmsAbvGrd','YearBuilt','YearRemodAdd','MasVnrArea']
sns.pairplot(train[cols], size = 2.5)
plt.show();

all_data=pd.concat([train,test])
all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)
all_data['OverallCond'] = all_data['OverallCond'].astype(str)
all_data['YrSold'] = all_data['YrSold'].astype(str)
all_data['MoSold'] = all_data['MoSold'].astype(str)


#categorical variable

cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond','ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', \
        'BsmtFinType1','BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope', \
        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \
        'YrSold', 'MoSold')
# process columns, apply LabelEncoder to categorical features, otherwise it will be treated as float
for c in cols:
    lbl = LabelEncoder() 
    lbl.fit(list(all_data[c].values)) 
    all_data[c] = lbl.transform(list(all_data[c].values))
#preprocessing.OneHotEncoder(sparse=False).fit_transform()
#get summy data for category data
all_data = pd.get_dummies(all_data, dummy_na=True)

#transfer the data for linear regression
#normlized the predictor and Saleprice
sns.distplot(all_data['SalePrice'],fit=norm)
fig=plt.figure()
stats.probplot(np.sqrt(train['SalePrice']), plot=plt)
# the plot is right skew
# transfer the sales to sqrt

all_data["SalePrice"] = np.log1p(all_data["SalePrice"])

# same with other variables, if we want to to linear regression, we need to make sure each variables are normal distr
numeric_feats = all_data.dtypes[all_data.dtypes != "object"].index
skewed=all_data[numeric_feats].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)

#change to log if right skew and power if left skew
all_data_lin["GrLivArea"] = np.log1p(all_data_lin["GrLivArea"])
all_data_lin["LotArea"] = np.log1p(all_data_lin["LotArea"])

all_data_lin["MiscVal"] = np.log1p(all_data_lin["MiscVal"])
all_data_lin["BsmtFinSF2"] = np.log1p(all_data_lin["BsmtFinSF2"])
all_data_lin["EnclosedPorch"] = np.log1p(all_data_lin["EnclosedPorch"])
all_data_lin["MasVnrArea"] = np.log1p(all_data_lin["MasVnrArea"])

all_data_lin["OpenPorchSF"] = np.log1p(all_data_lin["OpenPorchSF"])
all_data_lin["WoodDeckSF"] = np.log1p(all_data_lin["WoodDeckSF"])
all_data_lin["1stFlrSF"] = np.log1p(all_data_lin["1stFlrSF"])
all_data_lin["GrLivArea"] = np.log1p(all_data_lin["GrLivArea"])
all_data_lin["WoodDeckSF"] = np.log1p(all_data_lin["WoodDeckSF"])
all_data_lin["1stFlrSF"] = np.log1p(all_data_lin["1stFlrSF"])

car='MiscVal'
#from scipy.special import boxcox1p
sns.distplot(all_data[car],fit=norm)
sns.distplot(boxcox1p(all_data[car],0.15),fit=norm)
sns.distplot(np.log1p(all_data[car]),fit=norm)
LotArea
fig=plt.figure()
stats.probplot(np.log1p(train['SalePrice']), plot=plt)


#visulize the data
train=all_data[all_data["SalePrice"]>0]
test=all_data[all_data["SalePrice"].isnull()]
# Machine Learning Part
# Cross Validation on Random Forest to find best hyperparameter

# Number of trees in random forest
# Using Scikit-Learnâ€™s RandomizedSearchCV method, we can define a grid of hyperparameter ranges, 
#and randomly sample from the grid, performing K-Fold CV with each combination of values.
# use it to narrow down the hyparameter balue
#x_train, x_test, y_train, y_test = train_test_split(train, train.SalePrice,test_size=0.2, random_state=0)
X_train=train.drop(columns=['SalePrice'])
Y_train=train['SalePrice']
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)]
max_features = ['auto', 'sqrt']
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestRegressor()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
#
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
#rf_random =GridSearchCV(rf, random_grid, scoring='accuracy', cv=5, n_jobs=-1)
rf_random.fit(X_train, Y_train)
rf_random.best_params_

#Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search,
# we can explicitly specify every combination of settings to try. 
param_grid = {
    'bootstrap': [False],
    'max_depth': [80, 90, 100, 110,120,130],
    'max_features': [2, 3],
    'min_samples_leaf': [1,2,3, 4],
    'min_samples_split': [2,4,6,8, 10, 12],
    'n_estimators': [700, 800, 900, 1000]
}
# Create a based model
rf = RandomForestRegressor()
# Instantiate the grid search model
x_train, x_test, y_train_2, y_test = train_test_split(train, train.SalePrice,test_size=0.2, random_state=0)
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)
grid_search.fit(x_train, y_train_2)
grid_search.best_params_

def evaluate(model, test_features, test_labels,train_features, train_labels):
    predictions = model.predict(test_features)
    errors = abs(predictions - test_labels)
    mape = 100 * np.mean(errors / test_labels)
    accuracy = 100 - mape
    print('Model Performance')
    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))
    print('Accuracy = {:0.2f}%.'.format(accuracy))    
    print("The training error is: %.5f" % (1 - grid_search.score(train_features, train_labels)))
    print("The test     error is: %.5f" % (1 - grid_search.score(test_features, test_labels)))
    return accuracy

best_random = grid_search.best_estimator_
random_accuracy = evaluate(best_random, x_test, y_test,x_train,y_train_2)
#  Model Performance
#  Accuracy = 89.40%.
#  The training error is: 0.00000
#  The test     error is: 0.13696


#xgboost
xgboost_param_grid = {'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)],
              'learning_rate': [0.01, 0.02,0.03,0.04,0.05,0.06,0.07],
              'subsample': [0.3, 0.4,0.5,0.6, 0.7],
              'max_depth': [3, 4, 5, 6, 7, 8, 9],
              'min_child_weight': [1, 2, 3]
             }
xgb = xgboost.XGBRegressor()
clf = GridSearchCV(estimator=xgb, param_grid=xgboost_param_grid, cv=5, n_jobs=-1)
#clf = RandomizedSearchCV(xgb, param_distributions = xgboost_param_grid, n_iter = 25, scoring = 'f1', error_score = 0, verbose = 3, n_jobs = -1)
clf.fit(X_train, Y_train)
clf.best_score_




#